Big O Notation

It is a way to formalize fuzzy counting

It allows us to talk foramlly about how the runtime of an algorithm grows as the inputs grow


We say that an alrogirhm is O(f(n)) if the number of simple operations the computer has to do is eventually less than a constat times f(n),
as n increases

f(n) could be linear (f(n) = n)
  the first example with loop is f(n) = n
  So it is lenear

f(n) could be quadratic (f(n) = n^2)
f(n) could be constant (f(n) = 1) ==> addUpTofor example this func Always 
has 3 operations
function addUpTo(n) {
    return n * (n + 1) / 2
}
f(n) could be something entirely different


WHen determining the time complexiti of an 
algorithm, there are some helpful rule of
thumbs for big O expressions.

There rules of thumb are consequences of
the definiton of big O notation

O(2n) ==> O(n)
o(500) ==> O(1)
O(13n^2) ==> O(n^2)

1. Arithmetic operations are constant
2. Variable asisgnment is constant
3. accesing element in an array(by index) 
   or object(by key) is constant
4. In a loop the complexitiy is the length
of the loop times the complexity of whatever
happens inside of the loop


So far, we have been focusing on time complexity:
but how can we analyze the runtime of an algorithm
as the size of the input increases?

We can also use big O notation to analyze space
complexity: how much additional memory do we need 
to allocate in order to run the code in our algorithm?

Sometimes we will hear the themr auxiliary space
complexity to refer to space required by the
algorithm, iot including space taken up by the 
inputs.

unless othewrise noted, when wqe talk about space
complexity technically we will be talking about
auxilian space complexity


Rules Of thumb
Most primitives(booleans, numbers, undefined, null) are constant space
Strings require O(n) space (where n isthe string length)
Reference types are generally O(n) where n is the length (for arrays) or
the number ofd keys (for objects)


Who cares about logarithms complaxity?

1)Certain searching algorithms have logarthmic time complexity
2)Efficient sorting algorithm invlovle logarithms
3) Recursion sometimes involves logarithmic space complexity




--------------------Recap--------------------
To analyze the perfomance of an algorithm, we use Big O Notation
Big O Notation can give us a high level understanding of the time or space
complexity of an algorithm
Big O Notation doesnet care about the precision, only about general trends(
  linear? quadratic? constant?
)
The time or space complexity(as measured by Big O) depeneds only on the algorithm,
not the hardware used to run the algorithm



When to use objects?

When you dont need orders, because they are unordered data stuctures and they are
stored in key value pairs.
when you need fast access / insertion and removal

Big O of objects
Insertion - O(1)
removal - O(1)
searching - O(N)
Access - O(1)

When you dont need any ordering, objects are an excellent choice!

Object.keys - O(N)
Object.values - O(N)
Object.entries - O(N)
Object.hasOwnProperty - O(1)


When to use arrays?

When you need order
When you need fast access/ insertion and removal (sorf of....)

Insertion - it depeneds
removal - it depends
Searching - O(N)
Access - O(1)

We should be aware that adding and removing from start of array is not as Efficient
as deleting and adding elements in last. and not only that, most cases we must try
to avoid that

Push and pop is always faster then shift and unshift

push - 0(1)
pop - 0(1)
shift - 0(N)
unshift - 0(N)
concat - 0(N) merges multiple array together
slice - 0(N) it gonna return sahllow copy of array
splice - 0(N) it is going to remove and add new elements
sort - 0(N * log N)
forEach/map/filter/reduce/etc - 0(N)


-----------------------------------------------------

What is an algorithm?

A process or set of steps to accomplish a certain taks.

1) Understand the problem!
2) Explore Concrete Examples
3) Break It Down
4) Solve/ Simplify
5) Look Back and Refactor